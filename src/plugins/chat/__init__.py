from nonebot import on_message, on_command, get_bot
from nonebot.rule import to_me
from nonebot.adapters.onebot.v11 import GroupMessageEvent, PrivateMessageEvent, Bot
from nonebot.log import logger
import os
import json
from typing import Union

# Chat Handler
chat = on_message(priority=99, block=False)

# Clear command
clear_cmd = on_command("clear", aliases={"æ¸…ç©ºè®°å¿†"}, priority=5)

# Memory stats command
stats_cmd = on_command("memory", aliases={"è®°å¿†ç»Ÿè®¡"}, priority=5)

@stats_cmd.handle()
async def handle_stats(event: Union[GroupMessageEvent, PrivateMessageEvent]):
    from src.utils.conversation_memory import conversation_memory
    from src.utils.database import db
    
    stats = conversation_memory.get_stats()
    msg = (
        f"ğŸ“Š è®°å¿†ç»Ÿè®¡ï¼š\n"
        f"ğŸ‘¥ ç¼“å­˜ç”¨æˆ·æ•°ï¼š{stats['users_cached']}/{200}\n"
        f"ğŸ’¬ ä¸ªäººæ¶ˆæ¯æ•°ï¼š{stats['personal_messages']}\n"
        f"ğŸ˜ï¸ ç¾¤ä¸Šä¸‹æ–‡æ•°ï¼š{stats['group_contexts']}\n"
        f"ğŸ“ ç¾¤æ€»ç»“æ•°ï¼š{stats['total_summaries']}"
    )
    await stats_cmd.finish(msg)

@clear_cmd.handle()
async def handle_clear(event: Union[GroupMessageEvent, PrivateMessageEvent]):
    from src.utils.conversation_memory import conversation_memory
    
    # Get user identifier
    if isinstance(event, GroupMessageEvent):
        user_id = f"group_{event.group_id}_user_{event.user_id}"
    else:
        user_id = f"user_{event.user_id}"
    
    conversation_memory.clear_user(user_id)

    # Also clear this user's rows from group_context in current group
    if isinstance(event, GroupMessageEvent):
        try:
            db.clear_group_context_for_user(str(event.group_id), str(event.user_id))
        except Exception:
            pass

    await clear_cmd.finish("âœ… è®°å¿†å·²æ¸…ç©ºï¼Œæˆ‘ä»¬å¯ä»¥å¼€å§‹æ–°çš„å¯¹è¯äº†ï¼")

@chat.handle()
async def handle_chat(event: Union[GroupMessageEvent, PrivateMessageEvent]):
    try:
        # Check if message is to me
        if not event.is_tome():
            return

        # Import utilities
        from src.utils.conversation_memory import conversation_memory
        from src.utils.openai_client import openai_client
        from src.utils.image_utils import image_file_to_data_url
        from src.utils.message_parser import message_parser
        from src.utils.media_downloader import media_downloader
        
        # Parse message
        try:
            parsed = message_parser.parse_message(event)
        except Exception as e:
            logger.error(f"Message parsing failed: {e}")
            await chat.finish("æ¶ˆæ¯è§£æå¤±è´¥ï¼Œè¯·é‡è¯•ã€‚")
        # Media handling (OpenAI-compatible):
        # - Images: use vision endpoint
        # - Audio/Video: not supported yet
        if getattr(parsed, "has_media", False):
            if getattr(parsed, "audios", None) or getattr(parsed, "videos", None):
                await chat.finish("âš ï¸ æš‚ä¸æ”¯æŒè¯­éŸ³/è§†é¢‘è¾“å…¥ï¼ˆåç»­å¯åŠ æœ¬åœ° Whisper/TTSï¼‰ã€‚")
                return
            if getattr(parsed, "images", None):
                # Build vision prompt
                max_images = int(os.getenv("MAX_IMAGE_COUNT", "3"))
                max_px = int(os.getenv("IMAGE_MAX_PX", "1024"))
                quality = int(os.getenv("IMAGE_JPEG_QUALITY", "85"))

                image_urls = []
                for img in parsed.images[:max_images]:
                    if not img.url:
                        continue
                    file_path = await media_downloader.download_image(img.url, filename_hint=img.file)
                    image_urls.append(image_file_to_data_url(file_path, max_px=max_px, quality=quality))

                model_for_vision = os.getenv("MODEL_CHAT_LONG", os.getenv("MODEL_CHAT_SHORT", "auto"))
                reply = await openai_client.chat_completions_vision(
                    text_prompt=parsed.text or "è¯·æè¿°è¿™å¼ å›¾ç‰‡",
                    image_data_urls=image_urls,
                    model=model_for_vision,
                )

                from src.utils.text_formatter import markdown_to_plain_text
                reply = markdown_to_plain_text(reply)

                from src.utils.message_forwarder import send_message_smart
                threshold = int(os.getenv("FORWARD_THRESHOLD", "100"))

                try:
                    bot = get_bot()
                    await send_message_smart(bot=bot, message=reply, event=event, threshold=threshold)
                except Exception:
                    await chat.send(reply)

                await chat.finish()

            return

        # æ£€æŸ¥æ˜¯å¦ä¸ºå‘½ä»¤æ¶ˆæ¯ï¼ˆé¿å…ä¸å‘½ä»¤å¤„ç†å™¨å†²çªï¼‰
        if parsed.text:
            text_lower = parsed.text.strip().lower()
            # å®šä¹‰æ‰€æœ‰å‘½ä»¤å…³é”®è¯
            command_keywords = [
                'ping', 'åœ¨å—',
                'help', 'å¸®åŠ©', 'èœå•',
                'weather', 'å¤©æ°”',
                'add_rss', 'è®¢é˜…',
                'rss', 'è®¢é˜…åˆ—è¡¨', 'å–æ¶ˆè®¢é˜…',
                'rss_digest', 'ä»Šæ—¥æ‘˜è¦', 'rssæ‘˜è¦',
                'remind', 'æé†’',
                'summary', 'æ€»ç»“',
                'æ°´ç¾¤æ¦œ', 'èŠå¤©æ¦œ', 'å‘è¨€æ¦œ',
                'clear', 'æ¸…ç©ºè®°å¿†',
                'memory', 'è®°å¿†ç»Ÿè®¡',
                'db', 'æ•°æ®åº“'
            ]
            
            # æ£€æŸ¥æ¶ˆæ¯æ˜¯å¦ä»¥å‘½ä»¤å‰ç¼€å¼€å§‹æˆ–åŒ…å«å‘½ä»¤å…³é”®è¯
            # å»é™¤@æœºå™¨äººåçš„å†…å®¹è¿›è¡Œæ£€æŸ¥
            text_to_check = text_lower.strip()
            
            # å¦‚æœæ¶ˆæ¯ä»¥/å¼€å¤´ï¼Œç›´æ¥åˆ¤å®šä¸ºå‘½ä»¤
            if text_to_check.startswith('/'):
                logger.info(f"Skipping AI response for command: {text_to_check[:20]}")
                return
            
            # æ£€æŸ¥æ˜¯å¦å®Œå…¨åŒ¹é…å‘½ä»¤å…³é”®è¯æˆ–ä»¥å‘½ä»¤å…³é”®è¯å¼€å¤´
            for keyword in command_keywords:
                keyword_lower = keyword.lower()
                if text_to_check == keyword_lower or text_to_check.startswith(keyword_lower + ' '):
                    logger.info(f"Skipping AI response for command keyword: {keyword}")
                    return
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å†…å®¹
        if not parsed.text and not parsed.has_media:
            logger.warning("Empty message received (no text, no media)")
            await chat.finish("?")
        
        # Determine identifiers
        if isinstance(event, GroupMessageEvent):
            user_id = f"group_{event.group_id}_user_{event.user_id}"
            group_id = str(event.group_id)
            # ä¼˜å…ˆä½¿ç”¨ç¾¤åç‰‡
            user_name = event.sender.card or event.sender.nickname or str(event.user_id)
            
            # Add to group context (Tier 2)
            if parsed.text:
                try:
                    conversation_memory.add_group_context(group_id, str(event.user_id), user_name, parsed.text)
                except Exception as e:
                    logger.error(f"Failed to add group context: {e}")
        else:
            user_id = f"user_{event.user_id}"
            group_id = None
            user_name = None
        
        logger.info(f"Building context for {user_id}...")
        
        # Build full context (Tier 1 + Tier 2 + Tier 3)
        try:
            personal_history, system_context = conversation_memory.build_full_context(user_id, group_id)
        except Exception as e:
            logger.error(f"Failed to build context: {e}")
            personal_history = []
            system_context = None
        
        # å¤„ç†å¤šæ¨¡æ€å†…å®¹
        uploaded_files = []
        
        if parsed.has_media:
            # æ£€æŸ¥ç”¨æˆ·é…é¢
            from src.utils.quota_manager import quota_manager
            
            allowed, used, remaining = quota_manager.check_quota(user_id, is_multimodal=True)
            
            if not allowed:
                await chat.finish(
                    f"âš ï¸ æ‚¨ä»Šæ—¥çš„å¤šæ¨¡æ€åŠŸèƒ½ä½¿ç”¨æ¬¡æ•°å·²è¾¾ä¸Šé™ ({used}/{quota_manager.daily_limit})\n"
                    f"æ˜æ—¥0ç‚¹è‡ªåŠ¨é‡ç½®ï¼Œæˆ–ç»§ç»­ä½¿ç”¨çº¯æ–‡æœ¬å¯¹è¯ã€‚"
                )
            
            # å‰©ä½™æ¬¡æ•°è¾ƒå°‘æ—¶æé†’
            if remaining <= 5:
                logger.warning(f"User {user_id} has only {remaining} multimodal requests remaining today")
            
            logger.info(f"Processing multimodal message: {len(parsed.images)} images, "
                       f"{len(parsed.audios)} audios, {len(parsed.videos)} videos "
                       f"(quota: {used+1}/{quota_manager.daily_limit})")
            
            try:
                # ä¸‹è½½å¹¶ä¸Šä¼ å›¾ç‰‡
                from src.utils.image_compressor import image_compressor
                
                for idx, img in enumerate(parsed.images):
                    try:
                        logger.info(f"Processing image {idx+1}/{len(parsed.images)}: url={img.url[:80] if img.url else 'None'}, file={img.file[:50] if img.file else 'None'}")
                        
                        # æ£€æŸ¥URLæ˜¯å¦æœ‰æ•ˆ
                        if not img.url:
                            logger.error(f"Image {idx+1} has no URL")
                            continue
                        
                        #ä¸‹è½½å›¾ç‰‡
                        try:
                            file_path = await media_downloader.download_image(img.url, filename_hint=img.file)
                            logger.info(f"Image {idx+1} downloaded successfully: {file_path}")
                        except Exception as download_err:
                            logger.error(f"Failed to download image {idx+1}: {download_err}")
                            raise
                        
                        # å‹ç¼©å›¾ç‰‡
                        try:
                            compressed_path, was_compressed = image_compressor.compress_image(file_path)
                            if was_compressed:
                                logger.info(f"Image {idx+1} compressed: {compressed_path.name}")
                            else:
                                logger.info(f"Image {idx+1} kept original size: {compressed_path.name}")
                        except Exception as compress_err:
                            logger.error(f"Failed to compress image {idx+1}: {compress_err}")
                            raise
                        
                        # ä¸Šä¼ åˆ° Gemini
                        try:
                            mime_type = media_downloader.get_mime_type(compressed_path)
                            logger.info(f"Uploading image {idx+1} to Gemini (mime_type={mime_type})...")
                            uploaded = await openai_client.upload_file(compressed_path, mime_type)
                            uploaded_files.append(uploaded)
                            logger.info(f"Image {idx+1} uploaded successfully: {uploaded.name}")
                        except Exception as upload_err:
                            logger.error(f"Failed to upload image {idx+1} to Gemini: {upload_err}")
                            raise
                            
                    except Exception as e:
                        logger.error(f"Failed to process image {idx+1}: {type(e).__name__}: {str(e)}")
                        import traceback
                        logger.error(f"Traceback: {traceback.format_exc()}")
                
                # ä¸‹è½½å¹¶ä¸Šä¼ è¯­éŸ³
                for audio in parsed.audios:
                    try:
                        if not audio.url:
                            logger.warning(f"Audio has no URL: {audio.file}")
                            continue
                        
                        # ä¸‹è½½éŸ³é¢‘
                        file_path = await media_downloader.download_audio(audio.url)
                        
                        # è½¬æ¢æ ¼å¼ï¼ˆå¦‚æœéœ€è¦ï¼‰
                        from src.utils.audio_converter import audio_converter
                        
                        # QQ è¯­éŸ³é€šå¸¸æ˜¯ amr æˆ–å…¶ä»–æ ¼å¼ï¼Œè½¬æ¢ä¸º MP3
                        try:
                            converted_path = audio_converter.convert_to_mp3(file_path)
                            logger.info(f"Audio converted: {converted_path.name}")
                        except Exception as conv_err:
                            logger.warning(f"Audio conversion failed, using original: {conv_err}")
                            converted_path = file_path
                        
                        # ä¸Šä¼ åˆ° Gemini
                        mime_type = media_downloader.get_mime_type(converted_path)
                        uploaded = await openai_client.upload_file(converted_path, mime_type)
                        uploaded_files.append(uploaded)
                        logger.info(f"Audio uploaded: {uploaded.name}")
                    except Exception as e:
                        logger.error(f"Failed to process audio: {e}")
                
                # ä¸‹è½½å¹¶ä¸Šä¼ è§†é¢‘
                for video in parsed.videos:
                    try:
                        if not video.url:
                            logger.warning(f"Video has no URL: {video.file}")
                            continue
                        
                        file_path = await media_downloader.download_video(video.url)
                        mime_type = media_downloader.get_mime_type(file_path)
                        uploaded = await openai_client.upload_file(file_path, mime_type)
                        uploaded_files.append(uploaded)
                        logger.info(f"Video uploaded: {uploaded.name}")
                    except Exception as e:
                        logger.error(f"Failed to process video: {e}")
                        
            except Exception as e:
                logger.error(f"Error processing media: {e}")
                # ç»§ç»­å¤„ç†ï¼Œé™çº§ä¸ºçº¯æ–‡æœ¬
        
        # Construct System Prompt
        base_instruction = "è¯·æ³¨æ„ï¼šå•æ¡å›å¤å†…å®¹å°½é‡æ§åˆ¶åœ¨100ä¸ªä¸­æ–‡å­—ç¬¦ä»¥å†…ã€‚"
        
        if system_context:
            final_system_prompt = f"[ç³»ç»Ÿæç¤º]\n{system_context}\n\n{base_instruction}\nç°åœ¨å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œå¦‚æœç”¨æˆ·çš„é—®é¢˜ä¸ä¸Šä¸‹æ–‡ç›¸å…³ï¼Œè¯·ç»“åˆä¸Šä¸‹æ–‡å›ç­”ã€‚"
        else:
            final_system_prompt = f"[ç³»ç»Ÿæç¤º]\n{base_instruction}"
            
        system_msg = [{
            "role": "user", 
            "parts": [{"text": final_system_prompt}]
        }]
        
        # Prepend system message to history
        full_history = system_msg + personal_history
        
        logger.info(f"Chat from {user_id[:30]}... | history: {len(full_history)} | "
                   f"context: {'YES' if system_context else 'NO'} | "
                   f"media: {len(uploaded_files)}")
        
        # è°ƒç”¨ OpenAI-compatible API
        try:
            if uploaded_files:
                # å¤šæ¨¡æ€è°ƒç”¨
                # æ ¹æ®åª’ä½“ç±»å‹ç”Ÿæˆåˆé€‚çš„é»˜è®¤æç¤º
                if not parsed.text:
                    if parsed.audios:
                        text_prompt = "è¯·è½¬å½•è¿™æ®µè¯­éŸ³å¹¶å›ç­”å…¶ä¸­çš„é—®é¢˜ï¼ˆå¦‚æœæœ‰ï¼‰"
                    elif parsed.images:
                        text_prompt = "è¯·æè¿°å¹¶åˆ†æè¿™å¼ å›¾ç‰‡"
                    elif parsed.videos:
                        text_prompt = "è¯·æ€»ç»“è¿™ä¸ªè§†é¢‘çš„å†…å®¹"
                    else:
                        text_prompt = "è¯·åˆ†æè¿™ä¸ªå†…å®¹"
                else:
                    text_prompt = parsed.text
                
                # Force Flash or Pro for multimodal, Lite might not support it well or at all
                reply = await openai_client.generate_multimodal_content(
                    model='auto', 
                    text=text_prompt,
                    files=uploaded_files,
                    history=full_history,
                    task_type='chat'
                )
            else:
                # If user sent media but we failed to upload ANY of it
                if parsed.has_media:
                    await chat.finish("âš ï¸ æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ä¸‹è½½æˆ–å¤„ç†æ‚¨å‘é€çš„å›¾ç‰‡/åª’ä½“æ–‡ä»¶ã€‚å¯èƒ½æ˜¯ç½‘ç»œåŸå› æˆ–é“¾æ¥å¤±æ•ˆã€‚")
    
                # çº¯æ–‡æœ¬è°ƒç”¨
                reply = await openai_client.generate_content(
                    'auto', 
                    parsed.text, 
                    task_type='chat',
                    history=full_history
                )
        except Exception as e:
            logger.error(f"LLM API error: {e}")
            reply = "æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„æ¶ˆæ¯æ—¶å‡ºç°é”™è¯¯ã€‚"
        
        # è®°å½•é…é¢ä½¿ç”¨ï¼ˆæˆåŠŸè°ƒç”¨åï¼‰
        if uploaded_files:
            from src.utils.quota_manager import quota_manager
            quota_manager.use_quota(user_id, is_multimodal=True)
        
        # Convert Markdown to plain text for QQ compatibility
        from src.utils.text_formatter import markdown_to_plain_text
        reply = markdown_to_plain_text(reply)
        
        # Save to personal memory (Tier 1)
        try:
            conversation_memory.add_personal_message(user_id, "user", parsed.text or "[å¤šåª’ä½“å†…å®¹]")
            conversation_memory.add_personal_message(user_id, "model", reply)
        except Exception as e:
            logger.error(f"Failed to save conversation memory: {e}")
        
        logger.info(f"Reply: {reply[:80]}...")
        
        # ä½¿ç”¨æ™ºèƒ½å‘é€ï¼šè‡ªåŠ¨åˆ¤æ–­æ˜¯å¦éœ€è¦åˆå¹¶è½¬å‘
        from src.utils.message_forwarder import send_message_smart
        
        try:
            # è·å– Bot å®ä¾‹
            bot = get_bot()
            
            # è·å–è½¬å‘é˜ˆå€¼é…ç½®
            threshold = int(os.getenv("FORWARD_THRESHOLD", "100"))
            
            # æ™ºèƒ½å‘é€æ¶ˆæ¯
            await send_message_smart(
                bot=bot,
                message=reply,
                event=event,
                threshold=threshold
            )
        except Exception as e:
            logger.error(f"Failed to send message with smart forwarding: {e}")
            # é™çº§ä¸ºæ™®é€šå‘é€
            await chat.send(reply)
        
        # ç»“æŸå¯¹è¯
        await chat.finish()

    except Exception as e:
        # Ignore NoneBot's control flow exceptions
        from nonebot.exception import FinishedException
        if isinstance(e, FinishedException):
            raise e
            
        logger.error(f"Unexpected error in handle_chat: {e}")
        import traceback
        logger.error(traceback.format_exc())
        await chat.finish("ç³»ç»Ÿå‘ç”ŸæœªçŸ¥é”™è¯¯ï¼Œè¯·è”ç³»ç®¡ç†å‘˜ã€‚")

